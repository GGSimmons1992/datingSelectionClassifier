{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283e38b5-dbdb-48a3-901b-2d4c6fe803f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import exists\n",
    "from os import remove\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../util/\")\n",
    "import util as util\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f53352-c019-4b18-ad90-c9ec7e010d29",
   "metadata": {},
   "source": [
    "# Adding dummies and train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f609dd8-54ac-4fdf-929d-1c6870d95fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datingTrainWithDummiesExists = exists('../data/processedData/datingTrainWithDummies.csv')\n",
    "datingTestWithDummiesExists = exists('../data/processedData/datingTestWithDummies.csv')\n",
    "datingTrainWithoutDummiesExists = exists('../data/processedData/datingTrainWithoutDummies.csv')\n",
    "datingTestWithoutDummiesExists = exists('../data/processedData/datingTestWithoutDummies.csv')\n",
    "datingFullWithDummiesExists = exists('../data/processedData/datingFullWithDummies.csv')\n",
    "datingFullWithoutDummiesExists = exists('../data/processedData/datingFullWithoutDummies.csv')\n",
    "\n",
    "relatedDummiesDictionaryExists = exists('../data/processedData/relatedDummiesDictionary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bdd5e57-935f-47f6-baa4-506a1ec62765",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processedData/columnDataDictionary.json') as d:\n",
    "    columnDataDictionary = json.load(d)\n",
    "columnList = columnDataDictionary['columnList']\n",
    "nonBinaryCategoricalList = columnDataDictionary['nonBinaryCategoricalList']\n",
    "stringToFloatList = columnDataDictionary['stringToFloatList']\n",
    "pointDistributionList = columnDataDictionary['pointDistributionList']\n",
    "partnerList = columnDataDictionary['partnerList']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c38d144-9101-4d6a-802b-192a9d08ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garysimmons/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "if (datingTrainWithDummiesExists and datingTestWithDummiesExists and datingFullWithDummiesExists and\n",
    "    datingTrainWithoutDummiesExists and datingTestWithoutDummiesExists and datingFullWithoutDummiesExists and\n",
    "    relatedDummiesDictionaryExists):\n",
    "    datingTrainWithDummies = pd.read_csv('../data/processedData/datingTrainWithDummies.csv')\n",
    "    datingTestWithDummies = pd.read_csv('../data/processedData/datingTestWithDummies.csv')\n",
    "    datingTrainWithoutDummies = pd.read_csv('../data/processedData/datingTrainWithoutDummies.csv')\n",
    "    datingTestWithoutDummies = pd.read_csv('../data/processedData/datingTestWithoutDummies.csv')\n",
    "    datingFullWithDummies = pd.read_csv('../data/processedData/datingFullWithDummies.csv')\n",
    "    datingFullWithoutDummies = pd.read_csv('../data/processedData/datingFullWithoutDummies.csv')\n",
    "    \n",
    "    with open('../data/processedData/relatedDummiesDictionary.json') as d:\n",
    "        relatedDummiesDictionary = json.load(d)\n",
    "    for df in [datingTrainWithDummies,datingTestWithDummies,datingFullWithDummies,datingTrainWithoutDummies,datingTestWithoutDummies,datingFullWithoutDummies]:\n",
    "        df['zipcode'] = df['zipcode'].apply(str)\n",
    "        if 'zipcode_o' in list(df.columns):\n",
    "            df['zipcode_o'] = df['zipcode_o'].apply(str)\n",
    "        for col in nonBinaryCategoricalList:\n",
    "            if col in list(df.columns):\n",
    "                df[col] = df[col].apply(str)\n",
    "        \n",
    "else:\n",
    "    !rm -r ../data/processedData\n",
    "    !mkdir ../data/processedData\n",
    "    \n",
    "    datingData = pd.read_csv('../data/encoded-SpeedDatingData.csv')\n",
    "    blindDateData = datingData[columnList]\n",
    "    \n",
    "    for col in stringToFloatList:\n",
    "        blindDateData[col] = blindDateData[col].str.replace(',', '').astype(float)\n",
    "    blindDateData['zipcode'] = blindDateData['zipcode'].str.replace(',', '')\n",
    "    for col in nonBinaryCategoricalList:\n",
    "        blindDateData[col] = blindDateData[col].apply(str)\n",
    "    \n",
    "    blindDateCategoricalData = blindDateData.select_dtypes(include=['O'])\n",
    "    for col in blindDateCategoricalData.columns:\n",
    "        blindDateData[col]=blindDateData[col].fillna('nan')\n",
    "    relatedDummiesDictionary = {}\n",
    "    for col in blindDateCategoricalData.columns:\n",
    "        dummyData = pd.get_dummies(blindDateData[col],prefix=col,drop_first=True)\n",
    "        for dummyCol in dummyData.columns:\n",
    "            relatedDummiesDictionary[str(dummyCol)] = list(dummyData.columns)\n",
    "            if col in partnerList:\n",
    "                partnerList.append(str(dummyCol))\n",
    "        blindDateData = pd.concat([blindDateData,dummyData],axis=1)\n",
    "    with open('../data/processedData/relatedDummiesDictionary.json', 'w') as fp:\n",
    "        json.dump(relatedDummiesDictionary, fp)\n",
    "        \n",
    "    partnerList = list(set(partnerList))\n",
    "    columnDataDictionary = {\"columnList\": columnList,\n",
    "                        \"nonBinaryCategoricalList\": nonBinaryCategoricalList,\n",
    "                        \"stringToFloatList\": stringToFloatList,\n",
    "                        \"pointDistributionList\": pointDistributionList,\n",
    "                        \"partnerList\": partnerList}\n",
    "\n",
    "    with open('../data/processedData/columnDataDictionary.json', 'w') as fp:\n",
    "            json.dump(columnDataDictionary, fp)\n",
    "    \n",
    "    datingFullWithDummies = blindDateData.copy()\n",
    "    match = datingFullWithDummies['match']\n",
    "    X = datingFullWithDummies.drop(['match'], axis=1)\n",
    "    \n",
    "    datingTrainWithDummies, datingTestWithDummies, matchTrain, matchTest = train_test_split(X, match, test_size=0.2)\n",
    "    \n",
    "    datingTrainWithDummies['match'] = matchTrain\n",
    "    datingTestWithDummies['match'] = matchTest\n",
    "    \n",
    "    datingTrainWithDummies.to_csv('../data/processedData/datingTrainWithDummies.csv',index=False)\n",
    "    datingTestWithDummies.to_csv('../data/processedData/datingTestWithDummies.csv',index=False)\n",
    "    datingFullWithDummies.to_csv('../data/processedData/datingFullWithDummies.csv',index=False)\n",
    "    \n",
    "    dummyColumns = list(relatedDummiesDictionary.keys())\n",
    "    datingTrainWithoutDummies = datingTrainWithDummies.drop(dummyColumns, axis=1)\n",
    "    datingTestWithoutDummies = datingTestWithDummies.drop(dummyColumns, axis=1)\n",
    "    datingFullWithoutDummies = datingFullWithDummies.drop(dummyColumns, axis=1)\n",
    "    \n",
    "    datingTrainWithoutDummies.to_csv('../data/processedData/datingTrainWithoutDummies.csv',index=False)\n",
    "    datingTestWithoutDummies.to_csv('../data/processedData/datingTestWithoutDummies.csv',index=False)\n",
    "    datingFullWithoutDummies.to_csv('../data/processedData/datingFullWithoutDummies.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c695ed-1396-49fb-ae29-736e77d59d69",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd90ed-cccb-437b-a07e-1b153772db7c",
   "metadata": {},
   "source": [
    "### Join partner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3addb68-04fb-4afe-a7c8-dbd8d6825eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partnerCol in partnerList:\n",
    "    if ((\"_o\" not in str(partnerCol)) and (partnerCol+\"_o\" not in datingTrainWithoutDummies.columns) and (partnerCol+\"_o\" not in datingTrainWithDummies.columns)):\n",
    "        partnerWithDummies = datingFullWithDummies.copy()\n",
    "        partnerWithoutDummies = datingFullWithoutDummies.copy()\n",
    "        datingTrainWithDummies = util.joinToPartner(datingTrainWithDummies,partnerWithDummies)\n",
    "        datingTrainWithoutDummies = util.joinToPartner(datingTrainWithoutDummies,partnerWithoutDummies)\n",
    "        datingTrainWithDummies.to_csv('../data/processedData/datingTrainWithDummies.csv',index=False)\n",
    "        datingTrainWithoutDummies.to_csv('../data/processedData/datingTrainWithoutDummies.csv',index=False)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de70f81-b475-42fe-9dc7-8c107f2d28f8",
   "metadata": {},
   "source": [
    "### Get distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc4da6bb-d52b-4a7f-8ece-fe8de5dcea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((\"partnerDistance\" not in datingTrainWithoutDummies.columns) or \n",
    "    (\"partnerDistance\" not in datingTrainWithDummies.columns)):\n",
    "    datingTrainWithoutDummies = util.returnDFWithpartnerDistance(datingTrainWithoutDummies,\"train\",True)\n",
    "    locationColumns = [\"lats\",\"lons\",\"lats_o\",\"lons_o\",\"partnerDistance\"]\n",
    "    for locationCol in locationColumns:\n",
    "        datingTrainWithDummies[locationCol] = datingTrainWithoutDummies[locationCol]\n",
    "    datingTrainWithDummies.to_csv('../data/processedData/datingTrainWithDummies.csv',index=False)\n",
    "    datingTrainWithoutDummies.to_csv('../data/processedData/datingTrainWithoutDummies.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d629b9d5-3238-4fe4-bdc7-421fb93ea28d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fix ambiguous scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ede98e0-c807-43b3-976b-4728aece576b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pf_o_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pf_o_sum'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ec839993a9a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrowindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatingTrainWithDummies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatingTrainWithDummies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrowindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestionSumString\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestionSumString\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                     \u001b[0mquestionValues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrowindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestionCol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pf_o_sum'"
     ]
    }
   ],
   "source": [
    "halfwayChangeColumnsFromWithDummies = [str(col) for col in datingTrainWithDummies.columns if ((\"1_s\" in str(col)) | (\"3_s\" in str(col)))]\n",
    "halfwayChangeColumnsFromWithoutDummies = [str(col) for col in datingTrainWithoutDummies.columns if ((\"1_s\" in str(col)) | (\"3_s\" in str(col)))]\n",
    "\n",
    "if(len(halfwayChangeColumnsFromWithDummies) > 0):\n",
    "    for question in pointDistributionList:\n",
    "        questionSumString = f'{question}_sum'\n",
    "        questionCols = [str(col) for col in datingTrainWithDummies if question in str(col)]\n",
    "        datingTrainWithDummies[questionSumString] = datingTrainWithDummies[questionCols].sum(axis = 1)\n",
    "        datingTrainWithoutDummies[questionSumString] = datingTrainWithoutDummies[questionCols].sum(axis = 1)\n",
    "        for questionCol in questionCols:\n",
    "            questionValues = [np.nan] * datingTrainWithDummies.shape[0]\n",
    "            for rowindex in range(datingTrainWithDummies.shape[0]):\n",
    "                row = datingTrainWithDummies.iloc[rowindex]\n",
    "                if util.isNan(row[questionSumString]) | (row[questionSumString] == 0):\n",
    "                    questionValues[rowindex] = row[str(questionCol)]\n",
    "                else:\n",
    "                    row[str(questionCol)] * 100 / row[questionSumString]\n",
    "            datingTrainWithDummies[str(questionCol)] = pd.Series(questionValues)\n",
    "            datingTrainWithoutDummies[str(questionCol)] = pd.Series(questionValues)\n",
    "            datingTrainWithDummies = datingTrainWithDummies.drop(questionSumString,axis=1)\n",
    "            datingTrainWithoutDummies = datingTrainWithoutDummies.drop(questionSumString,axis=1)\n",
    "    for halfwayQuestion in halfwayChangeColumnsFromWithoutDummies:\n",
    "        targetQuestion = \"\"\n",
    "        if (\"1_s\" in halfwayQuestion):\n",
    "            if halfwayQuestion == \"attr1_s_o\":\n",
    "                targetQuestion = \"pf_o_att\"\n",
    "            elif halfwayQuestion == \"sinc1_s_o\":\n",
    "                targetQuestion = \"pf_o_sin\"\n",
    "            elif halfwayQuestion == \"intel1_s_o\":\n",
    "                targetQuestion = \"pf_o_int\"\n",
    "            elif halfwayQuestion == \"fun1_s_o\":\n",
    "                targetQuestion = \"pf_o_fun\"\n",
    "            elif halfwayQuestion == \"amb1_s_o\":\n",
    "                targetQuestion = \"pf_o_amb\"\n",
    "            elif halfwayQuestion == \"shar1_s_o\":\n",
    "                targetQuestion = \"pf_o_sha\"\n",
    "            else:\n",
    "                targetQuestion = halfwayQuestion.replace(\"1_s\",\"1_1\")\n",
    "        else:\n",
    "            targetQuestion = halfwayQuestion.replace(\"3_s\",\"3_1\")\n",
    "        currentMindsetAnswers = []\n",
    "        for rowindex in range(datingTrainWithDummies.shape[0]):\n",
    "            row = datingTrainWithDummies.iloc[rowindex]\n",
    "            if util.isNan(row[halfwayQuestion]) | (row[\"order\"] <= int(row[\"round\"])):\n",
    "                currentMindsetAnswers.append(row[targetQuestion])\n",
    "            else:\n",
    "                currentMindsetAnswers.append(row[halfwayQuestion])\n",
    "        datingTrainWithDummies[targetQuestion] = pd.Series(currentMindsetAnswers)\n",
    "        datingTrainWithoutDummies[targetQuestion] = pd.Series(currentMindsetAnswers)\n",
    "        datingTrainWithDummies = datingTrainWithDummies = datingTrainWithDummies.drop(halfwayQuestion,axis = 1)\n",
    "        datingTrainWithoutDummies = datingTrainWithoutDummies.drop(halfwayQuestion,axis = 1)\n",
    "    datingTrainWithDummies.to_csv('../data/processedData/datingTrainWithDummies.csv',index=False)\n",
    "    datingTrainWithoutDummies.to_csv('../data/processedData/datingTrainWithoutDummies.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4444c2d-2bae-4e16-848d-5035c7700c62",
   "metadata": {},
   "source": [
    "### Replace Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1fe4d-f74a-4a20-a5e5-9924a44d97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datingTrainNumerical = datingTrainWithDummies.select_dtypes(include=['uint8','int64','float64'])\n",
    "\n",
    "if (exists('../data/processedData/trainNanReplacementValuesDictionary.json')):\n",
    "    with open('../data/processedData/trainNanReplacementValuesDictionary.json') as d:\n",
    "        trainNanReplacementValuesDictionary = json.load(d)\n",
    "else:\n",
    "    trainNanReplacementValuesDictionary = {}\n",
    "    for col in datingTrainNumerical:\n",
    "        if col in [\"int_corr\",\"age\",\"age_o\",\"pf_o_att\",\"pf_o_sin\",\"pf_o_int\",\"pf_o_fun\",\"pf_o_amb\",\"pf_o_sha\",\"attr_o\",\"sinc_o\",\"intel_o\",\"amb_o\",\"fun_o\",\"shar_o\",\n",
    "                  \"like_o\",\"prob_o\",\"mn_sat\",\"tuition\",\"income\",\"sports\",\"tvsports\",\"exercise\",\"dining\",\"museums\",\"art\",\"hiking\",\"gaming\",\"reading\",\"tv\",\n",
    "                  \"theater\",\"movies\",\"concerts\",\"music\",\"shopping\",\"yoga\",\"exphappy\",\"expnum\",\"attr1_1\",\"sinc1_1\",\"intel1_1\",\"fun1_1\",\"shar1_1\",\"attr4_1\",\"sinc4_1\",\n",
    "                  \"intel4_1\",\"fun4_1\",\"shar4_1\",\"attr2_1\",\"sinc2_1\",\"intel2_1\",\"fun2_1\",\"shar2_1\",\"attr3_1\",\"sinc3_1\",\"intel3_1\",\"fun3_1\",\"attr5_1\",\"sinc5_1\",\n",
    "                   \"intel5_1\",\"fun5_1\",\"attr\",\"sinc\",\"intel\",\"fun\",\"amb\",\"like\",\"prob\",\"match_es\",\"sports_o\",\"tvsports_o\",\"exercise_o\",\"dining_o\",\"museums_o\",\"art_o\",\n",
    "                  \"hiking_o\",\"gaming_o\",\"clubbing_o\",\"reading_o\",\"tv_o\",\"theater_o\",\"movies_o\",\"concerts_o\",\"music_o\",\"shopping_o\",\"yoga_o\",\"exphappy_o\",\"expnum_o\",\n",
    "                   \"attr4_1_o\",\"sinc4_1_o\",\"intel4_1_o\",\"fun4_1_o\",\"shar4_1_o\",\"attr2_1_o\",\"sinc2_1_o\",\"intel2_1_o\",\"fun2_1_o\",\"shar2_1_o\",\"attr3_1_o\",\"sinc3_1_o\",\n",
    "                   \"intel3_1_o\",\"fun3_1_o\",\"attr5_1_o\",\"sinc5_1_o\",\"intel5_1_o\",\"fun5_1_o\",\"match_es_o\",\"lats\",\"lons\",\"lats_o\",\"lons_o\",\"partnerDistance\"]:\n",
    "            trainNanReplacementValuesDictionary[str(col)] = datingTrainNumerical[col].mean()\n",
    "        elif col in [\"imprace\",\"imprelig\",\"zipcode\",\"goal\",\"date\",\"go_out\",\"career_c\",\"met\",\"imprace_o\",\"imprelig_o\",\"zipcode_o\",\"goal_o\",\"date_o\",\"career_c_o\"]:\n",
    "            trainNanReplacementValuesDictionary[str(col)] = round(np.mean(datingTrainNumerical[col].mode().values))\n",
    "        else:\n",
    "            trainNanReplacementValuesDictionary[str(col)] = 0\n",
    "    with open('../data/processedData/trainNanReplacementValuesDictionary.json', 'w') as fp:\n",
    "        json.dump(trainNanReplacementValuesDictionary, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d48406-ee7c-4e34-b985-f0b2a325363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datingTrainWithDummies = util.replaceNansWithTrainingDataValues(datingTrainWithDummies)\n",
    "datingTrainWithoutDummies = util.replaceNansWithTrainingDataValues(datingTrainWithoutDummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911fcbe2-48f5-470d-8dbf-d59feb48cec9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf544a-51dc-4d1d-9619-654cc93ac168",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5508ffa-dea4-49d3-8d1c-333f80a4f625",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973841e-f4c8-465c-b56a-8d23896d59dc",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c1e00-aac9-487b-92b6-b0766b1ab8b9",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0d71d-dbd6-4075-8f9e-fc57118d0ee4",
   "metadata": {},
   "source": [
    "# Individual Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e41fa1-50ce-4fd6-85b3-c9ec47b37a53",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7aec2-8e5e-4b55-8feb-c22af75e031d",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77cabc1-2855-4ac3-987f-61546d0b7da8",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2830fb95-9857-4d16-b538-62c025b07b47",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa7875-d2a8-4e24-aa49-25d005b1e8a5",
   "metadata": {},
   "source": [
    "# Ensemble Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
