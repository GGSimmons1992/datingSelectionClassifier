{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e38b5-dbdb-48a3-901b-2d4c6fe803f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import exists\n",
    "from os import remove\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.model_selection import cross_validate\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.ensemble import GradientBoostingClassifier as grad\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import metrics\n",
    "import scipy.stats as stats\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../util/\")\n",
    "import util as util\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f53352-c019-4b18-ad90-c9ec7e010d29",
   "metadata": {},
   "source": [
    "# Adding dummies and train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f609dd8-54ac-4fdf-929d-1c6870d95fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datingTrainExists = exists('../data/processedData/datingTrain.csv')\n",
    "datingTestExists = exists('../data/processedData/datingTest.csv')\n",
    "datingFullExists = exists('../data/processedData/datingFull.csv')\n",
    "\n",
    "relatedDummiesDictionaryExists = exists('../data/processedData/relatedDummiesDictionary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd5e57-935f-47f6-baa4-506a1ec62765",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists('../data/processedData/columnDataDictionary.json'):\n",
    "    with open('../data/processedData/columnDataDictionary.json') as d:\n",
    "        columnDataDictionary = json.load(d)\n",
    "else:\n",
    "    with open('../data/columnDataDictionary.json') as d:\n",
    "        columnDataDictionary = json.load(d)\n",
    "columnList = columnDataDictionary['columnList']\n",
    "nonBinaryCategoricalList = columnDataDictionary['nonBinaryCategoricalList']\n",
    "stringToFloatList = columnDataDictionary['stringToFloatList']\n",
    "pointDistributionList = columnDataDictionary['pointDistributionList']\n",
    "partnerList = columnDataDictionary['partnerList']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c38d144-9101-4d6a-802b-192a9d08ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (datingTrainExists and datingTestExists and datingFullExists and relatedDummiesDictionaryExists):\n",
    "    datingTrain = pd.read_csv('../data/processedData/datingTrain.csv')\n",
    "    datingTest = pd.read_csv('../data/processedData/datingTest.csv')\n",
    "    datingFull = pd.read_csv('../data/processedData/datingFull.csv')\n",
    "    \n",
    "    with open('../data/processedData/relatedDummiesDictionary.json') as d:\n",
    "        relatedDummiesDictionary = json.load(d)\n",
    "    for df in [datingTrain,datingTest,datingFull]:\n",
    "        df['zipcode'] = df['zipcode'].apply(str)\n",
    "        if 'zipcode_o' in list(df.columns):\n",
    "            df['zipcode_o'] = df['zipcode_o'].apply(str)\n",
    "        for col in nonBinaryCategoricalList:\n",
    "            if col in list(df.columns):\n",
    "                df[col] = df[col].apply(str)\n",
    "        \n",
    "else:\n",
    "    !rm -r ../data/processedData\n",
    "    !mkdir ../data/processedData\n",
    "    \n",
    "    datingData = pd.read_csv('../data/encoded-SpeedDatingData-WithLocations.csv')   \n",
    "    \n",
    "    blindDateData = datingData[columnList]\n",
    "    \n",
    "    for col in stringToFloatList:\n",
    "        blindDateData[col] = blindDateData[col].str.replace(',', '').astype(float)\n",
    "    \n",
    "    blindDateData['zipcode'] = blindDateData['zipcode'].apply(str)\n",
    "    blindDateData['zipcode'] = blindDateData['zipcode'].str.replace(',', '')\n",
    "    \n",
    "    for col in nonBinaryCategoricalList:\n",
    "        blindDateData[col] = blindDateData[col].apply(str)\n",
    "    \n",
    "    blindDateCategoricalData = blindDateData.select_dtypes(include=['O'])\n",
    "    for col in blindDateCategoricalData.columns:\n",
    "        blindDateData[col]=blindDateData[col].fillna('nan')\n",
    "    relatedDummiesDictionary = {}\n",
    "    for col in blindDateCategoricalData.columns:\n",
    "        dummyData = pd.get_dummies(blindDateData[col],prefix=col,drop_first=True)\n",
    "        if len(dummyData.columns) <= 25:\n",
    "            for dummyCol in dummyData.columns:\n",
    "                relatedDummiesDictionary[str(dummyCol)] = list(dummyData.columns)\n",
    "                if col in partnerList:\n",
    "                    partnerList.append(str(dummyCol))\n",
    "                    partnerDummies = [partnerdummy+\"_o\" for partnerdummy in list(dummyData.columns)]\n",
    "                    relatedDummiesDictionary[str(dummyCol)+\"_o\"] = partnerDummies\n",
    "            blindDateData = pd.concat([blindDateData,dummyData],axis=1)\n",
    "    with open('../data/processedData/relatedDummiesDictionary.json', 'w') as fp:\n",
    "        json.dump(relatedDummiesDictionary, fp)\n",
    "        \n",
    "    partnerList = list(set(partnerList))\n",
    "    columnDataDictionary = {\"columnList\": columnList,\n",
    "                        \"nonBinaryCategoricalList\": nonBinaryCategoricalList,\n",
    "                        \"stringToFloatList\": stringToFloatList,\n",
    "                        \"pointDistributionList\": pointDistributionList,\n",
    "                        \"partnerList\": partnerList}\n",
    "\n",
    "    with open('../data/processedData/columnDataDictionary.json', 'w') as fp:\n",
    "            json.dump(columnDataDictionary, fp)\n",
    "    \n",
    "    datingFull = blindDateData.copy()\n",
    "    match = datingFull['match']\n",
    "    X = datingFull.drop(['match'], axis=1)\n",
    "    \n",
    "    datingTrain, datingTest, matchTrain, matchTest = train_test_split(X, match, test_size=0.2)\n",
    "    \n",
    "    datingTrain['match'] = matchTrain\n",
    "    datingTest['match'] = matchTest\n",
    "    \n",
    "    datingTrain.to_csv('../data/processedData/datingTrain.csv',index=False)\n",
    "    datingTest.to_csv('../data/processedData/datingTest.csv',index=False)\n",
    "    datingFull.to_csv('../data/processedData/datingFull.csv',index=False)\n",
    "    \n",
    "    dummyColumns = list(relatedDummiesDictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c9fd3-e2eb-479b-94e6-336f5b622152",
   "metadata": {},
   "outputs": [],
   "source": [
    "match = datingTrain[\"match\"]\n",
    "matchTest = datingTest[\"match\"]\n",
    "datingTrain = datingTrain.drop(\"match\",axis=1)\n",
    "datingTest = datingTest.drop(\"match\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c695ed-1396-49fb-ae29-736e77d59d69",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd90ed-cccb-437b-a07e-1b153772db7c",
   "metadata": {},
   "source": [
    "### Join partner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3addb68-04fb-4afe-a7c8-dbd8d6825eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\"iid\" in datingTrain.columns) or (\"pid\" in datingTrain.columns):\n",
    "    partner = datingFull.copy()\n",
    "    datingTrain = util.joinToPartner(datingTrain,partner).drop([\"iid\",\"pid\",\"iid_o\",\"pid_o\"],axis=1)\n",
    "    datingTrain.to_csv('../data/processedData/datingTrain.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de70f81-b475-42fe-9dc7-8c107f2d28f8",
   "metadata": {},
   "source": [
    "### Get distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4da6bb-d52b-4a7f-8ece-fe8de5dcea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"partnerDistance\" not in datingTrain.columns:\n",
    "    datingTrain = util.returnDFWithpartnerDistance(datingTrain,\"train\",True)\n",
    "    datingTrain.to_csv('../data/processedData/datingTrain.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d629b9d5-3238-4fe4-bdc7-421fb93ea28d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fix ambiguous scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede98e0-c807-43b3-976b-4728aece576b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "halfwayChangeColumns = [str(col) for col in datingTrain.columns if ((\"1_s\" in str(col)) | (\"3_s\" in str(col)))]\n",
    "\n",
    "if(len(halfwayChangeColumns) > 0):\n",
    "    datingTrain = util.fixAmbiguousScores(datingTrain)\n",
    "    util.halfwayQuestionSanityTest(datingTrain,\" post-fixAmbiguousScores and pre-saving\")\n",
    "    datingTrain.to_csv('../data/processedData/datingTrain.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4444c2d-2bae-4e16-848d-5035c7700c62",
   "metadata": {},
   "source": [
    "### Replace Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1fe4d-f74a-4a20-a5e5-9924a44d97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datingTrainNumerical = datingTrain.select_dtypes(include=['uint8','int64','float64'])\n",
    "\n",
    "if (exists('../data/processedData/trainNanReplacementValuesDictionary.json')):\n",
    "    with open('../data/processedData/trainNanReplacementValuesDictionary.json') as d:\n",
    "        trainNanReplacementValuesDictionary = json.load(d)\n",
    "else:\n",
    "    trainNanReplacementValuesDictionary = {}\n",
    "    for col in datingTrainNumerical:   \n",
    "        if len(list(set(datingTrainNumerical[col]))) <=30:\n",
    "            setValues = [val for val in list(set(datingTrainNumerical[col])) if ((np.isnan(val) == False) and np.isfinite(val))]\n",
    "            setValues = pd.Series(setValues)\n",
    "            trainNanReplacementValuesDictionary[str(col)] = round(np.mean(setValues.mode().values))\n",
    "        else:\n",
    "            trainNanReplacementValuesDictionary[str(col)] = datingTrainNumerical[col].mean()\n",
    "    with open('../data/processedData/trainNanReplacementValuesDictionary.json', 'w') as fp:\n",
    "        json.dump(trainNanReplacementValuesDictionary, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d48406-ee7c-4e34-b985-f0b2a325363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = util.replaceNansWithTrainingDataValues(datingTrainNumerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911fcbe2-48f5-470d-8dbf-d59feb48cec9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248dff16-8447-41c3-9aa4-01d8f87f90e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrtn = int(np.sqrt(X.shape[0]))\n",
    "sqrtfeatures = int(np.sqrt(X.shape[1]))\n",
    "log2features = int(np.log2(X.shape[1]))\n",
    "midpoint = int((sqrtfeatures + log2features)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf544a-51dc-4d1d-9619-654cc93ac168",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692996cb-15e6-4e17-82b4-aded2aaa7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "logModel = lm.LogisticRegression(max_iter=1e9)\n",
    "logPipe = make_pipeline(StandardScaler(), logModel)\n",
    "logPipe.fit(X,match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be89e99-53e8-4a66-bcfd-01d4d04631b1",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaae2c2-a197-4be1-879d-dae4c408dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn5 = knn(n_neighbors=5)\n",
    "knnsqrtn = knn(n_neighbors=sqrtn)\n",
    "knn5.fit(X,match)\n",
    "knnsqrtn.fit(X,match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c1e00-aac9-487b-92b6-b0766b1ab8b9",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa52d8bd-bca9-4366-91c9-6ae12753e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientdeci = grad(learning_rate=0.1)\n",
    "gradientdeka = grad(learning_rate=10)\n",
    "\n",
    "gradientdeci.fit(X,match)\n",
    "gradientdeka.fit(X,match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6304b01-3857-4bd0-ae45-1d9831a1d334",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec3947-934b-42ae-a30b-99d4b32d7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists(\"../data/processedData/forestParams.json\"):\n",
    "    with open('../data/processedData/forestParams.json') as d:\n",
    "        forestParams = json.load(d)\n",
    "        preciseForestParams = forestParams[\"preciseForestParams\"]\n",
    "        recallForestParams = forestParams[\"recallForestParams\"]\n",
    "else:\n",
    "    print(f\"sqrt(nFeatures) ~ {sqrtfeatures}\")\n",
    "    print(f\"log2(nFeatures) ~ {log2features}\")\n",
    "    print(f\"midpoint = {midpoint}\")\n",
    "    searchParams = {\n",
    "        \"criterion\": [\"gini\",\"entropy\",\"log_loss\"],\n",
    "        \"n_estimators\": [100,200,300],\n",
    "        \"max_depth\":[sqrtfeatures,midpoint,log2features,None],\n",
    "        \"max_features\":[sqrtfeatures,midpoint,log2features,None]\n",
    "    }\n",
    "\n",
    "    preciseForest0 = rf()\n",
    "    recallForest0 = rf()\n",
    "    \n",
    "    try:\n",
    "        recallForestGrid = ms.GridSearchCV(recallForest0, param_grid=searchParams, scoring='recall',n_jobs=5)\n",
    "        recallForestGrid.fit(X,match)\n",
    "        recallForestParams = recallForestGrid.best_params_\n",
    "        print(\"recall params:\")\n",
    "        print(recallForestParams)\n",
    "        \n",
    "        for key in searchParams.keys():\n",
    "            searchParams[key] = [val for val in searchParams[key] if val != recallForestParams[key]]\n",
    "        \n",
    "        preciseForestGrid = ms.GridSearchCV(accurateForest0, param_grid=searchParams, scoring='precision',n_jobs=5)\n",
    "        preciseForestGrid.fit(X,match)\n",
    "        preciseForestParams = accurateForestGrid.best_params_\n",
    "        print(\"precision params:\")\n",
    "        print(preciseForestParams)\n",
    "        \n",
    "        \n",
    "        forestParams = {\n",
    "            \"preciseForestParams\": preciseForestParams,\n",
    "            \"recallForestParams\": recallForestParams\n",
    "        }\n",
    "        with open(\"../data/processedData/forestParams.json\", 'w') as fp:\n",
    "            json.dump(forestParams, fp)\n",
    "    except BaseException:\n",
    "        util.displayValueExceptionColumn(X)\n",
    "    \n",
    "if exists(\"../data/processedData/forestParams.json\"):\n",
    "    preciseForest = rf(n_estimators = preciseForestParams[\"n_estimators\"],\n",
    "                    criterion = preciseForestParams[\"criterion\"],\n",
    "                    max_depth = preciseForestParams[\"max_depth\"],\n",
    "                    max_features = preciseForestParams[\"max_features\"])\n",
    "    recallForest = rf(n_estimators = recallForestParams[\"n_estimators\"],\n",
    "                    criterion = recallForestParams[\"criterion\"],\n",
    "                    max_depth = recallForestParams[\"max_depth\"],\n",
    "                    max_features = recallForestParams[\"max_features\"])\n",
    "\n",
    "preciseForest.fit(X,match)\n",
    "recallForest.fit(X,match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b7521-ae27-4bc6-927a-bebc5afa9385",
   "metadata": {},
   "source": [
    "# Top 10 Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0c9389-fc66-43c4-8147-2f810bc0417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "XColumns = list(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff56188-6bf0-4303-8140-d5c357b3c832",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b5cd9-6b50-4e37-a180-fcffa3af0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = [coef for coef in logPipe.named_steps['logisticregression'].coef_.reshape(-1,)]\n",
    "absCoefficients = [absCoef for absCoef in np.abs(np.array(coefficients))]\n",
    "\n",
    "logImportances = pd.DataFrame({\n",
    "    \"feature\": XColumns,\n",
    "    \"coefficients\": coefficients,\n",
    "    \"absCoefficients\": absCoefficients},columns = [\"feature\",\"coefficients\",\"absCoefficients\"])\n",
    "logImportancesSorted = logImportances.sort_values(by=\"absCoefficients\", ascending=False)\n",
    "print(f'logistic regression top 10 feature importances')\n",
    "for i in range(10):\n",
    "    featureRow = logImportancesSorted.iloc[i]\n",
    "    feature = featureRow['feature']\n",
    "    featureValue = featureRow['coefficients']\n",
    "    print(f'Rank {i}: {feature}: score: {featureValue}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7d40c-18da-4212-9b76-e0a6234ec217",
   "metadata": {},
   "source": [
    "## KNN focuses on nearest neighbors, not on specific features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57094aa-a7e8-4e93-b9e6-cf907e6792dc",
   "metadata": {},
   "source": [
    "## Gradient Boosting & Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5f9e2-be43-4967-b461-c44109712eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [gradientdeci,gradientdeka,preciseForest,recallForest]\n",
    "modelNames = [\"gradientdeci\",\"gradientdeka\",\"preciseForest\",\"recallForest\"]\n",
    "\n",
    "for i in range(4):\n",
    "    util.displayFeatureImportances(XColumns,models[i],modelNames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0d71d-dbd6-4075-8f9e-fc57118d0ee4",
   "metadata": {},
   "source": [
    "# Individual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8f07d-229a-42d4-b80c-fdc367a5b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\"iid\" in datingTest.columns) or (\"pid\" in datingTest.columns):\n",
    "    partner = datingFull.copy()\n",
    "    datingTest = util.joinToPartner(datingTest,partner).drop([\"iid\",\"pid\",\"iid_o\",\"pid_o\"],axis=1)\n",
    "    datingTest.to_csv('../data/processedData/datingTest.csv',index=False)\n",
    "\n",
    "if \"partnerDistance\" not in datingTest.columns:\n",
    "    datingTest = util.returnDFWithpartnerDistance(datingTest,\"test\",True)\n",
    "    datingTest.to_csv('../data/processedData/datingTest.csv',index=False)\n",
    "\n",
    "halfwayChangeColumns = [str(col) for col in datingTest.columns if ((\"1_s\" in str(col)) | (\"3_s\" in str(col)))]\n",
    "\n",
    "if(len(halfwayChangeColumns) > 0):\n",
    "    datingTest = util.fixAmbiguousScores(datingTest)\n",
    "    util.halfwayQuestionSanityTest(datingTest,\" post-fixAmbiguousScores and pre-saving\")\n",
    "    datingTest.to_csv('../data/processedData/datingTest.csv',index=False)\n",
    "\n",
    "datingTestNumerical = datingTest.select_dtypes(include=['uint8','int64','float64'])\n",
    "XTest = util.replaceNansWithTrainingDataValues(datingTestNumerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e41fa1-50ce-4fd6-85b3-c9ec47b37a53",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de6d4f-9cf1-4fe1-a8a4-7518399c3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "yPredict = logPipe.predict(XTest)\n",
    "util.displayMetricScores(matchTest,yPredict,\"logPipe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7aec2-8e5e-4b55-8feb-c22af75e031d",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740813a8-47bd-4aac-aa99-33e740762a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn5.predict(XTest)\n",
    "util.displayMetricScores(matchTest,yPredict,\"knn5\")\n",
    "knnsqrtn.predict(XTest)\n",
    "util.displayMetricScores(matchTest,yPredict,\"knnsqrtn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2830fb95-9857-4d16-b538-62c025b07b47",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381d498-e009-4abb-b879-0464d405650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientdeci.predict(XTest)\n",
    "util.displayMetricScores(matchTest,yPredict,\"gradientdeci\")\n",
    "gradientdeka.predict(XTest)\n",
    "util.displayScores(matchTest,yPredict,\"gradientdeka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d72fb6-44f4-4e30-8e43-02c8133a28c0",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861a9a7-c1a2-47b5-af90-d3e1a928512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preciseForest.predict(XTest)\n",
    "util.displayMetricScores(matchTest,yPredict,\"preciseForest\")\n",
    "recallForest.predict(XTest)\n",
    "util.displayMetricScores(matchTest,yPredict,\"recallForest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa7875-d2a8-4e24-aa49-25d005b1e8a5",
   "metadata": {},
   "source": [
    "# Ensemble Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3242b1-66fa-4d7a-81f7-4555e4431702",
   "metadata": {},
   "outputs": [],
   "source": [
    "logModel = lm.LogisticRegression(max_iter=1e9)\n",
    "logPipe = make_pipeline(StandardScaler(), logModel)\n",
    "knn5 = knn(n_neighbors=5)\n",
    "knnsqrtn = knn(n_neighbors=sqrtn)\n",
    "gradientdeci = grad(learning_rate=0.1)\n",
    "gradientdeka = grad(learning_rate=10)\n",
    "preciseForest = rf(n_estimators = preciseForestParams[\"n_estimators\"],\n",
    "                    criterion = preciseForestParams[\"criterion\"],\n",
    "                    max_depth = preciseForestParams[\"max_depth\"],\n",
    "                    max_features = preciseForestParams[\"max_features\"])\n",
    "recallForest = rf(n_estimators = recallForestParams[\"n_estimators\"],\n",
    "                  criterion = recallForestParams[\"criterion\"],\n",
    "                  max_depth = recallForestParams[\"max_depth\"],\n",
    "                  max_features = recallForestParams[\"max_features\"])\n",
    "\n",
    "ensembleVote = VotingClassifier(\n",
    "    estimators = [\n",
    "        (\"logModel\",logPipe),\n",
    "        (\"knn5\",knn5),\n",
    "        (\"knnsqrtn\",knnsqrtn),\n",
    "        (\"gradientdeci\",gradientdeci),\n",
    "        (\"gradientdeka\",gradientdeka),\n",
    "        (\"preciseForest\",preciseForest),\n",
    "        (\"recallForest\",recallForest)\n",
    "    ]\n",
    ")\n",
    "\n",
    "ensembleVote.fit(X,match)\n",
    "ensembleDecision = ensemble.predict(XTest)\n",
    "util.displayMetricScores(matchTest,ensembleDecision,\"Ensemble\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
